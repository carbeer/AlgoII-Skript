\chapter{Parallele Algorithmen}

\section{Einleitung}

Indem man Parallelverarbeitung verwendet, kann man sowohl Ressourcen einsparen als auch Ressourcenrestriktionen brechen:
\begin{itemize}
  \item \textbf{Zeitersparnis}: Arbeiten \( p \) Computer an einem Problem, so sind sie bis zu \( p \) mal so schnell.
  \item \textbf{Kommunikationsersparnis}: Fallen Daten verteilt an, so kann man sie auch verteilt (vor)verarbeiten.
  \item \textbf{Energieersparnis}: Zwei Prozessoren mit halber Taktfrequenz brauchen weniger Energie als ein voll getakteter Prozessor.
  \item \textbf{Speicherbeschränkung}: Mehr Prozessoren haben mehr Hauptspeicher, mehr Cache,\dots
\end{itemize}

Es gibt sehr viele Modelle der Parallelverarbeitung, wir werden hier allerdings nur zwei Standardmodelle diskutieren:
\begin{itemize}
  \item \textbf{Prozessornetzwerke mit} \term{Nachrichtenkopplung}\index{Nachrichtenkopplung}.

    Prozessoren sind hier ``normale CPUs'' mit lokalem Speicher. Gemeinsam mit seinem lokalen Speicher wird eine lokale CPU auch \emph{processing element} (PE) genannt. Der Datenaustausch passiert über ein Netzwerk in Form von Nachrichten zwischen zwei Prozessoren.
  \item \textbf{Parallele Registermaschinen mit} \term{Speicherkopplung}\index{Speicherkopplung}.

    Auch hier wird mit normalen CPUs gearbeitet, allerdings haben diese keinen lokalen Speicher, sondern sind über ein Netzwerk mit Speichermodulen verbunden, auf welche alle CPUs zugreifen können. Der Datenaustausch erfolgt über das Netzwerk zwischen einer CPU und einem Speicher.
\end{itemize}

Auf nachrichtengekoppelte Rechner werden wir genauer eingehen.

\begin{figure}[H]
  \includegraphics[width=.8\textwidth]{ParallelArchitectures}
  \captionsetup{width=.8\textwidth}
  \caption{Vergleich zwischen \emph{parallelen Registermaschinen mit Speicherkopplung} (links) und einem \emph{Prozessornetzwerk mit Nachrichtenkopplung} (rechts)}
\end{figure}

\subsection{Nachrichtenkopplung vs. Speicherkopplung}

Neben den oben erläuterten Vorteilen von Parallelverarbeitung haben beide Modelle auch Nachteile:

\begin{itemize}
  \item \textbf{Nachrichtenkopplung}:
  \begin{itemize}
    \item Zwei Prozessoren werden zum Datentransport benötigt. Das passt dem Empfänger nicht immer.
    \item Parallelismus muss explizit programmiert werden.
  \end{itemize}
  \item \textbf{Speicherkopplung}:
  \begin{itemize}
    \item \emph{Skalierbarkeit}: Ist eine große Anzahl an Prozessoren sinnvoll?
    \item \emph{Kostenmaß} bei Speicherzugriffskonflikten?
  \end{itemize}
\end{itemize}

Als eine gute Strategie hat es sich erwiesen, den Entwurf für einen verteilten Speicher durchzuführen, da dieser einen viel breiteren Bereich abdecken kann. Die Implementierung erfolgt dann gegebenenfalls für einen gemeinsamen Speicher.

\section{Nachrichtengekoppelte Parallelrechner}

\subsection{Modell}

\begin{itemize}
  \item \textbf{Netzwerk}: Vollständig verknüpftes Punkt-zu-Punkt-Netzwerk
  \begin{itemize}
    \item voll-duplex
    \item Nachrichten überholen sich nicht
  \end{itemize}
  \item \textbf{Prozessoren}: RAMs, können jeweils maximal gleichzeitig
  \begin{itemize}
    \item eine Nachricht an einen beliebigen Empfänger senden (send(smsg,to))
    \item eine Nachricht von einem beliebigen Sender empfangen (rmsg \( \coloneqq \) recv(from))
    \item \emph{oder} beides gleichzeitig (rmsg \( \coloneqq \) sendRecv(smsg, to, from))
  \end{itemize}
\end{itemize}

Als \emph{Kostenmodell} für das Senden oder Empfangen von \( l \) Bytes verwenden wir
\begin{equation*}
  T_\text{comm}(l) = T_\text{start} + l*T_\text{byte}\text{,}
\end{equation*}
wobei in der Praxis meist \( T_\text{byte} \ll T_\text{start} \). Ignoriert wird hier unter anderem der ``Abstand'' zwischen Sender und Empfänger.

Als \emph{Programmiermodell} verwenden wir \term{SPMD}\index{SPMD} (\emph{single program multiple data}). Alle PEs führen hier dasselbe Programm aus, unterschieden wird (zur Symmetriebrechung) häufig durch ``Ränge'' der PEs (paarweise verschiedene PE-Nummern).

\subsection{Analyse}
\begin{itemize}
	\item Ausführungszeit: $T(p)$. 
	\item Arbeit: $W=p\cdot T(p)$. Ist ein Kostenmaß.
	\item Span: $T_\infty = min_p T(p)$. Misst Parallelisierbarkeit. Für beliebig viele Prozessoren der kleinste Wert.
	\item Absoluter Speedup: $S=\frac{T_Seq}{T(p)}$. Entspricht der Beschleunigung gegenüber dem besten sequenziellen Algorithmus. 
	\item Relativer Speedup: $\frac{T(1)}{T(p)}$. 
	\item Effizienz: $E=\frac{S}{p}$. Superlinearer Speeup für relative Beschleunigung über 1, idR. nicht möglich (außer wenn plötzlich alles in den Cache passt, usw.). Ziel ist es aber generell Werte nahe $1$ oder in $\Theta(1)$ zu erreichen. 
\end{itemize}


\subsection{Parallele Reduktion}

Im Folgenden gehen wir über die grundlegenden Werkzeuge, die wir benötigen, um parallele Programme analysieren zu können.

\begin{definition}[Reduktion]
  Sei \( \otimes \) eine binäre, assoziative Operation auf einer Menge \( M \).

  Für \( x = (x_0,\dots,x_{p-1}) \in M \) definieren wir
  \begin{equation*}
    R_\otimes(x) = \bigotimes_{i < p}x_i = x_0 \otimes \cdots \otimes x_{p-1}\text{.}
  \end{equation*}
\end{definition}

Nun gilt folgender Satz:

\begin{theorem}
  Wenn \( \otimes \) eine assoziative Operation ist, welche sich in konstanter Zeit berechnen lässt, und \( p \) Elemente \( x_0,\dots,x_{p-1} \) auf \( p \) PEs verteilt sind, dann kann man \( \bigotimes_{i < p}x_i \) in Zeit \( O(\log p) \) auf PE \( 0 \) berechnen. 
\end{theorem}

\begin{figure}[H]
  \includegraphics[width=.5\textwidth]{parallelExecution}
  \caption{Idee hinter obigem Satz}
\end{figure}

Sequenziell wäre die optimale Laufzeit \( O(n) \). Auf \( p = n \) PEs hätte man nach obigem Satz eine Laufzeit von \( O(\log n) \) erreicht, also eine Beschleunigung von \( O\left( \frac{n}{\log n} \right) \). ``Ideal'' wäre eine Beschleunigung von \( O(p) = O(n) \).

Wie kann man die Beschleunigung noch weiter verbessern?

\subsection{Parallele Reduktion mit \( p < n \)}

Verwenden wir nun \( p < n \) viele PEs, um eine Reduktion auf \( n \) Elementen durchzuführen, so erhält jedes PE \( n/p \) Datenelemente. Die PEs berechnen zuerst die Reduktion der lokalen Elemente, und anschließend wird auf diesen Reduktionen eine parallele Reduktion durchgeführt.
Laufzeit hierfür ist \( O(n/p) + O(\log p) \) und die Beschleunigung somit
\begin{equation*}
  \frac{O(n)}{O(n/p + \log p)}\text{.}
\end{equation*}
Ist \( p \in O(n/\log n) \), so ist die Beschleunigung \( O(p) \).

Man kann also durch Verringerung der Prozessorzahl \( p \) die Beschleunigung in die Nähe von \( p \) bringen. Ineffiziente Algorithmen werden also durch Verringerung der Prozessorzahl effizient. Dieses Prinzip nennt man \term{Brent's Prinzip}\index{Brent's Prinzip}.


\subsection{Parallele Präfixsummen}

Wir verwenden \( \otimes \) wie oben definiert. Wir definieren nun
\begin{align*}
  &P_\otimes(x) = y = (y_0,\dots,y_{p-1}) \\
  \text{mit } &y_i = \bigotimes_{k \leq i}x_k\text{.} %\text{ oder } y_i = \bigotimes_{k < i}x_k \text{ falls neutrales Element \( e \).}
\end{align*}
Es ist also \( y_0 = x_0 \) und \( y_{i+1} = y_i \otimes x_{i+1} \).

\subsection{Präfixsummen --- Hyperwürfel-Algorithmus}

Wir machen es uns hier einfach und legen fest, dass \( p = n = 2^d \) (für \( d \in \N \)) und \( \otimes \) kommutativ.

Jede PE erhält nun eine ``Koordinate'' \( 0 \leq i \leq 2^d-1 \). Diese wird als Bitvektor
\begin{equation*}
  i = (i_{d-1}\cdots i_0) \quad \text{mit} \quad i_j \in \left \{ 0,1 \right \}
\end{equation*}
repräsentiert. Hier ist \( i_k \) das \( k \)-te Bit von rechts in \( i \).

Wir erlauben Kommunikation zwischen zwei PE \( i \) und \( i' \) nur, wenn die Hammingdistanz ihrer Bitvektoren \( 1 \) ist, sie sich also nur in einer Stelle unterscheiden. Wir erhalten so einen \emph{Hyperwürfel} aus PEs.

Wir berechnen nun Präfixsummen auf einem solchen Hyperwürfel:

\begin{pseudocode}
  \textbf{\textsc{prefixSum}}\( (x,\otimes) \) \\
  \( y \coloneqq x \) \enskip \textcolor{gray}{// auf PE \( i \) liegt \( x_i \)} \\
  \( s \coloneqq x \) \enskip \textcolor{gray}{// für Summe von Elementen in Unterwürfel} \\
  \textbf{for} \( k \coloneqq 0 \) \textbf{to} \( d - 1 \) \textbf{do} \\
  \phantom{\enskip} \( s' \coloneqq \textsc{sendRecv}(s,i \oplus 2^k, i \oplus 2^k) \) \\
  \phantom{\enskip} \( s \coloneqq s \otimes s' \) \\
  \phantom{\enskip} \textbf{if} \( i_k \equiv 1 \) \textbf{then} \( y \coloneqq y \otimes s' \) 
  \textcolor{gray}{// auf PE \( i \) liegt \( y_i = x_0 \otimes \cdots \otimes x_i \)}
\end{pseudocode}

Wir erhalten eine Laufzeit
\begin{equation*}
  T_\text{prefix} \in O((T_\text{start} + l * T_\text{byte}) * \log p)\text{.}
\end{equation*}

Diese Laufzeit ist nicht optimal für $l * T_\text{byte} > T_\text{start}$. Wie man sie optimieren kann wird in der Vorlesung ``Parallele Algorithmen'' näher erläutert.

\subsection{Paralleles Sortieren}

Hier gibt es zwei verschiedene Aufgabenvarianten:

\begin{enumerate}
  \item Alle \( n \) Elemente liegen zu Beginn auf PE 0. Deswegen muss jedes Element von PE 0 mindestens einmal angefasst werden. Die Laufzeit ist daher in \( \Omega(n) \).
  \item Je \( n/p \) Elemente liegen zu Beginn auf PE \( i \). Dieser Fall ist wesentlich interessanter.
\end{enumerate}

Zunächst betrachten wir den einfachen Fall \( p = n \) (Prozessor \( i \) hat Eingabeelement \( x_i \)). Wir behalten die Grundidee von Quicksort bei:
\begin{itemize}
  \item Wir wählen ein Element pv als Pivot.
  \item Elemente werden umverteilt:
  \begin{itemize}
    \item kleiner als pv: auf Prozessoren mit kleineren Rängen
    \item größer als pv: auf Prozessoren mit großen Rängen
  \end{itemize}
  \item Parallele Rekursion.
\end{itemize}

Wir schauen uns den \term{Theoretiker-Quicksort}\index{Theoretiker-Quicksort} an:

\begin{pseudocode}
  \textcolor{gray}{// Teil 0: Vorbereitungen} \\
  \textbf{\textsc{theoQSort0}}\( (x) \) \\
  \( i \in {0,p-1} \)  \enskip \textcolor{gray}{// hier hat PE \( i \) Element \( x_i \)} \\
  \( p \coloneqq \text{Anzahl aller PEs} \)\\
  \( \text{theoQSort}(x,i,p) \) \\
  \ \\
  \textcolor{gray}{// Teil 1: kleine Elemente zählen} \\
  \textbf{\textsc{theoQSort}}\( (x,\dots) \) \\
  \textbf{if} \( p = 1 \) \textbf{then} \( \text{return} \) \\
  \(r \coloneqq rand(0,p-1) \) \enskip \textcolor{gray}{// derselbe Wert in der gesamten Partition} \\
  \( \text{pv} = d@r\) \enskip \textcolor{gray}{broadcast value of pivot}\\
  \( \text{small} \coloneqq (d \leq \text{pv}) \) \\
  \( j \coloneqq \text{prefixSum}(\text{small},0,i) \) \\
  \( p' \coloneqq \text{bcast}(j,p-1) \)\enskip \textcolor{gray}{$p'$ ist der border index} \\
  \ \\
  \textcolor{gray}{// Teil 2: Datenumverteilung und Rekursion} \\
  \textbf{if} \( \text{small} \equiv 1 \) \textbf{then} \( \text{send}(d,j-1) \) \\
  \textbf{else} \( \text{send}(d,p'+i-j) \) \\
  \( d \coloneqq \text{recv}() \) \\
  recursive theoQSort of ``left''/``right'' part
\end{pseudocode}

Die erwartete Rekursionstiefe ist in \( O(\log p) \), die Zeit jeweils in \( O(T_\text{start}\log p) \). Insgesamt ist die erwartete Zeit also in \( O(T_\text{start}(\log p)^2) \).